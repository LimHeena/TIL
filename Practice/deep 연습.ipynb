{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ratsnlp.nlpbook.generation import GenerationTrainArguments\n",
    "args = GenerationTrainArguments(\n",
    "    pretrained_model_name=\"skt/kogpt2-base-v2\",\n",
    "    downstream_corpus_name=\"nsmc\",\n",
    "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-generation\",\n",
    "    max_seq_length=32,\n",
    "    batch_size=32 if torch.cuda.is_available() else 4,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=3,\n",
    "    tpu_cores=0 if torch.cuda.is_available() else 8,\n",
    "    seed=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed: 7\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp import nlpbook\n",
    "nlpbook.set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpbook.set_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:01, 7.48MB/s]                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 6.27MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=args.force_download,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 2.83M/2.83M [00:00<00:00, 24.4MB/s]\n",
      "Downloading: 100%|██████████| 1.00k/1.00k [00:00<00:00, 478kB/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    eos_token=\"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.generation import NsmcCorpus, GenerationDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "corpus = NsmcCorpus()\n",
    "train_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\",\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"test\",\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 513M/513M [00:18<00:00, 27.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    args.pretrained_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "TPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mratsnlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnlpbook\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgeneration\u001b[39;00m \u001b[39mimport\u001b[39;00m GenerationTask\n\u001b[0;32m      2\u001b[0m task \u001b[39m=\u001b[39m GenerationTask(model, args)\n\u001b[1;32m----> 3\u001b[0m trainer \u001b[39m=\u001b[39m nlpbook\u001b[39m.\u001b[39;49mget_trainer(args)\n",
      "File \u001b[1;32mc:\\Users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages\\ratsnlp\\nlpbook\\trainer.py:17\u001b[0m, in \u001b[0;36mget_trainer\u001b[1;34m(args, return_trainer_only)\u001b[0m\n\u001b[0;32m      9\u001b[0m os\u001b[39m.\u001b[39mmakedirs(ckpt_path, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[0;32m     11\u001b[0m     dirpath\u001b[39m=\u001b[39mckpt_path,\n\u001b[0;32m     12\u001b[0m     save_top_k\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39msave_top_k,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{epoch}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{val_loss:.2f}\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     18\u001b[0m     max_epochs\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mepochs,\n\u001b[0;32m     19\u001b[0m     fast_dev_run\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtest_mode,\n\u001b[0;32m     20\u001b[0m     num_sanity_val_steps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m args\u001b[39m.\u001b[39;49mtest_mode \u001b[39melse\u001b[39;49;00m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m     21\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback],\n\u001b[0;32m     22\u001b[0m     default_root_dir\u001b[39m=\u001b[39;49mckpt_path,\n\u001b[0;32m     23\u001b[0m     \u001b[39m# For GPU Setup\u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m     deterministic\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39mand\u001b[39;49;00m args\u001b[39m.\u001b[39;49mseed \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     25\u001b[0m     gpus\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mdevice_count() \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     26\u001b[0m     precision\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m \u001b[39mif\u001b[39;49;00m args\u001b[39m.\u001b[39;49mfp16 \u001b[39melse\u001b[39;49;00m \u001b[39m32\u001b[39;49m,\n\u001b[0;32m     27\u001b[0m     \u001b[39m# For TPU Setup\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m     tpu_cores\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtpu_cores \u001b[39mif\u001b[39;49;00m args\u001b[39m.\u001b[39;49mtpu_cores \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[39mif\u001b[39;00m return_trainer_only:\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer\n",
      "File \u001b[1;32mc:\\Users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:339\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[0;32m    338\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:483\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n\u001b[1;32m--> 483\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[0;32m    484\u001b[0m     num_processes\u001b[39m=\u001b[39;49mnum_processes,\n\u001b[0;32m    485\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[0;32m    486\u001b[0m     tpu_cores\u001b[39m=\u001b[39;49mtpu_cores,\n\u001b[0;32m    487\u001b[0m     ipus\u001b[39m=\u001b[39;49mipus,\n\u001b[0;32m    488\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[0;32m    489\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[0;32m    490\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[0;32m    491\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[0;32m    492\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[0;32m    493\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[0;32m    494\u001b[0m     replace_sampler_ddp\u001b[39m=\u001b[39;49mreplace_sampler_ddp,\n\u001b[0;32m    495\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[0;32m    496\u001b[0m     auto_select_gpus\u001b[39m=\u001b[39;49mauto_select_gpus,\n\u001b[0;32m    497\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[0;32m    498\u001b[0m     amp_type\u001b[39m=\u001b[39;49mamp_backend,\n\u001b[0;32m    499\u001b[0m     amp_level\u001b[39m=\u001b[39;49mamp_level,\n\u001b[0;32m    500\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[0;32m    501\u001b[0m )\n\u001b[0;32m    502\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m LoggerConnector(\u001b[39mself\u001b[39m, log_gpu_memory)\n\u001b[0;32m    503\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:196\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_accelerator()\n\u001b[1;32m--> 196\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_parallel_devices_and_init_accelerator()\n\u001b[0;32m    198\u001b[0m \u001b[39m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_and_init_cluster_environment()\n",
      "File \u001b[1;32mc:\\Users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:514\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m    511\u001b[0m     available_accelerator \u001b[39m=\u001b[39m [\n\u001b[0;32m    512\u001b[0m         acc_str \u001b[39mfor\u001b[39;00m acc_str \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_types \u001b[39mif\u001b[39;00m AcceleratorRegistry\u001b[39m.\u001b[39mget(acc_str)\u001b[39m.\u001b[39mis_available()\n\u001b[0;32m    513\u001b[0m     ]\n\u001b[1;32m--> 514\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    515\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m can not run on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    516\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    517\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    518\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer`: \u001b[39m\u001b[39m{\u001b[39;00mavailable_accelerator\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_devices_flag_if_auto_passed()\n\u001b[0;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_devices_flag \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: TPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "from ratsnlp.nlpbook.generation import GenerationTask\n",
    "task = GenerationTask(model, args)\n",
    "trainer = nlpbook.get_trainer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (1.6.1)\n",
      "Collecting pytorch-lightning\n",
      "  Using cached pytorch_lightning-2.0.2-py3-none-any.whl (719 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (2.0.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (2023.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (0.11.4)\n",
      "Requirement already satisfied: packaging>=17.1 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (4.5.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.7.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from pytorch-lightning) (0.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.29.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (3.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dlagm\\anaconda3\\envs\\deep\\lib\\site-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
      "Installing collected packages: pytorch-lightning\n",
      "  Attempting uninstall: pytorch-lightning\n",
      "    Found existing installation: pytorch-lightning 1.6.1\n",
      "    Uninstalling pytorch-lightning-1.6.1:\n",
      "      Successfully uninstalled pytorch-lightning-1.6.1\n",
      "Successfully installed pytorch-lightning-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ratsnlp 1.0.52 requires pytorch-lightning==1.6.1, but you have pytorch-lightning 2.0.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning --upgrade\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
